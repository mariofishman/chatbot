# Connective Logbook

## Intro

This logbook documents the end-to-end development of **Connective**, an intelligent networking assistant designed to capture, organize, and advance professional relationships through a conversational interface. The system blends React-based frontend UI with an SSE-driven assistant layer and a FastAPI backend, orchestrated via LangGraph and modularized across Contacts, Follow-Ups, and Goals. Its architecture is tailored for memory-rich interactions and strategic relationship tracking, supported by Tailwind v4, shadcn/ui, and Python-based services.

Each log entry captures the concrete progress made across frontend, backend, AI orchestration, and UI design systems, reflecting every decision, implementation milestone, and architectural shift. This record serves both as an engineering history and a design reference as the platform evolves toward full feature parity and deployment.

The project is grounded in atomic design principles and follows a tightly organized development sequence, blending interface engineering with assistant orchestration via LangGraph. All changes, adjustments to strategy, and implementation notes are recorded here to ensure continuity, collaboration, and maintainability across development phases. The logbook begins post-environment setup and continues through to deployment and polish.

## üìÖ Log Entry: November 23, 2025

As of this entry, the first 12 steps of the Connective roadmap have been completed. The logbook begins here, retroactively summarizing progress but formally starting **after Step 12** (backend scaffolding and setup).

### Original Roadmap and Adjustments

The Connective project began with a clear 30-step roadmap outlining the sequential phases of frontend setup, chat UI, backend integration, assistant orchestration, and deployment. As development progressed, some steps were clarified, modified, or replaced to better fit emerging needs‚Äîsuch as removing Subframe, enhancing Tailwind v4 support, and adding retry handling.

### Original 30-Step Roadmap

1. Install and set up VSCode (extensions, settings, workspace basics)
2. Install and set up Cursor (AI-assisted coding)
3. Create repo and initialize Git workspace
4. Scaffold frontend with Vite + React + TypeScript
5. Install and configure Tailwind CSS (ensure v4)
6. Install and configure shadcn/ui with Tailwind v4 compatibility  
   6b. Set up shadcn MCP server for Cursor integration
7. Build global UI shell (header, container, chat layout)
8. Implement chat message components and input bar
9. Add SSE client hook (EventSource) with polling fallback
10. Draft custom widgets in Subframe and export React + Tailwind
11. Refactor Subframe exports into shadcn conventions
12. Scaffold FastAPI backend with Uvicorn and CORS enabled
13. Define Pydantic settings and environment management
14. Implement `/chat` SSE endpoint (token streaming)
15. Implement `/summary` endpoint (JSON request/response)
16. Wire LLM provider (OpenAI-compatible) and client in Python
17. Create LangChain/LangGraph ‚Äúinterview ‚Üí summary‚Äù flow
18. Design SQLModel models (Session, Message, CompanyProfile, Summary)
19. Implement persistence services and CRUD functions
20. Stream-and-save messages during `/chat` interaction
21. Connect frontend API clients to backend endpoints
22. Add anonymous session handling and ID propagation
23. Configure local dev (Vite proxy, CORS, env vars)
24. Write minimal tests (Vitest/React Testing Library for frontend, Pytest for backend)
25. Deploy frontend to Vercel/Netlify
26. Deploy backend to Fly.io/Render
27. Set production config (CORS, HTTPS, SSE stability, env secrets)
28. Add observability (structured logs, request IDs, monitoring)
29. Polish UI/UX (styling, animations, responsive tweaks)  
    29b. [Added] Error-Recovery and Retry Handling
30. Security hardening (rate limits, input validation, error handling)

### Changes Made

- **Subframe removed**: Steps 10‚Äì11 now involve direct widget creation in Cursor using Tailwind and shadcn/ui.
- **Tailwind v4 support**: Explicit postcss patches and adjustments required.
- **SSE clarified**: Stream opens per message, not persistent.
- **New step 29b**: Retry logic and assistant error handling.

### Updated Roadmap (Post-Revisions)

This updated roadmap reflects all changes made during early implementation, incorporating removed tools, clarified setups, and added error-handling logic. It tracks sequential development across frontend, backend, assistant logic, and deployment.

1. **VSCode setup**
2. **Cursor setup**
3. **Git repo initialization**
4. **Scaffold frontend with Vite + React + TypeScript**
5. **Configure Tailwind CSS v4** (with `@tailwindcss/postcss` support)
6. **Install shadcn/ui** (Tailwind v4 compatible)  
   6b. **Set up shadcn MCP server** for Cursor CLI integration
7. **Build global UI shell** (header, layout, theming)
8. **Implement chat message system**
   - 8a. Message model + mock data
   - 8b. Message bubble component
   - 8c. Chat scroll + rendering
   - 8d. Input area (composer)
   - 8e. Centralized state
   - 8f. Local mock only
   - 8g. SSE scaffolding
9. **Add SSE streaming client logic**
   - 9a. EventSource utility
   - 9b. Message lifecycle tie-in
   - 9c. Token streaming to UI
   - 9d. Error handling
   - 9e. Stream cleanup
   - 9f. Mock streaming mode
10. **Draft custom widgets directly in Cursor** using shadcn and Tailwind
11. **Refactor widgets to shadcn/ui conventions** (componentized, theme-aligned)
12. **Scaffold FastAPI backend**

- 12.1. Create backend folder + virtual environment
- 12.2. Install FastAPI, Uvicorn, and dependencies
- 12.3. Set up main app + `/health` route
- 12.4. Configure CORS
- 12.5. Structure `app/`, `routers/`, `config/`
- 12.6. Add dev script
- 12.7. Verify with browser/Postman
- 12.8. Document run instructions in README

13. **Define Pydantic `BaseSettings`** config and `.env` loading
14. **Implement `/chat` SSE endpoint** for assistant messages
15. **Implement `/summary` endpoint** for structured data
16. **Wire LLM provider** (e.g., OpenAI-compatible)
17. **Build LangGraph agent** for interview-to-summary flow
18. **Design SQLModel data models** (`Session`, `Message`, `Contact`, `Goal`, etc.)
19. **Implement database services and persistence logic**
20. **Stream and save messages live during `/chat`**
21. **Connect frontend to backend endpoints** via fetch/SWR
22. **Handle anonymous sessions and session propagation**
23. **Configure full-stack local dev (proxy, CORS, .env)**
24. **Write frontend/backend unit tests**
25. **Deploy frontend (Vercel/Netlify)**
26. **Deploy backend (Fly.io/Render)**
27. **Set production config and CORS/security headers**
28. **Add logs, monitoring, and observability hooks**
29. **Polish UI (responsive layout, animation, UX polish)**

- 29b. **[New] Error-Recovery + Retry Handling**

30. **Harden security** (input validation, rate limiting, etc.)

## üìÖ Log Entry: November 23, 2025

**Step 12 ‚Äì Backend Scaffold Completed**

The FastAPI backend was successfully scaffolded, completing all subtasks under Step 12. This foundational backend setup enables future API development, assistant orchestration, and persistent data handling.

### ‚úÖ What was accomplished:

1. Created the `backend/` project folder and initialized a virtual environment.
2. Installed core dependencies: **FastAPI**, **Uvicorn**, and supporting packages.
3. Set up the **main FastAPI app file** (`main.py`) with a working `GET /health` route.
4. Configured **CORS middleware** with allowed origins matching the frontend.
5. Defined a clean **project structure**, including `app/`, `routers/`, and `config/` modules.
6. Added a **development script** (`run_dev.sh`) for launching Uvicorn with auto-reload.
7. Manually tested the health endpoint via browser and/or Postman to confirm server response.
8. Wrote clear **instructions in `README.md`**, including dependency installation, virtualenv activation, and run commands.
9. Cleaned up `.gitignore` and verified that all files are properly tracked and versioned.

The backend now runs in development mode with hot reload, correctly handles CORS, and serves as the entry point for future routes like `/chat` and `/summary`.

### The following are sub-steps for roadmap step 13:

13.1 Create `.env` and `.env.example` files
13.2 Define a `Settings` Pydantic model (BaseSettings)
13.3 Add strongly-typed fields (API keys, DB URL, CORS origins)
13.4 Load settings in a central `config/settings.py` module
13.5 Reference settings from `main.py` (CORS + future usage)
13.6 Add automatic environment loading (python-dotenv)
13.7 Add validation and defaults for missing variables
13.8 Update README with environment variable instructions

## üìÖ Log Entry: November 23, 2025

**Step 13 ‚Äì Pydantic Settings & Environment Management Completed**

This step transitioned the backend from a static configuration to a robust, environment-driven service using **Pydantic v2** and the `pydantic-settings` package. The configuration is now centralized, type-safe, validated, and automatically loaded from a `.env` file.

### ‚úÖ What was accomplished:

- **Created**: `.env`, `.env.example`
- **Implemented**: `Settings` model in `config/settings.py` using `BaseSettings`
- **Configured fields**: `openai_api_key`, `database_url`, `cors_origins` (with validator), `llm_model`, `llm_temperature`, `debug`
- **Enabled automatic loading**: with `model_config` set to read `.env`
- **Updated**: `main.py` to load settings and apply dynamic CORS values
- **Validated inputs**: e.g., converting comma-separated CORS strings into lists
- **Documented**: usage and required variables in `README.md`

This setup will enable future backend modules to securely and consistently access configuration values for database connectivity, LLM behavior, session control, and deployment tuning‚Äîall from a single source of truth.

---

### The following are sub-steps for roadmap step 14:

14.1 Define `/chat` request and response schema (Pydantic models)
14.2 Design minimal in-memory message/session model for backend
14.3 Implement core chat handler function (non-streaming)
14.4 Add SSE response helper (generator that yields `data:` chunks)
14.5 Implement `/chat` SSE endpoint using FastAPI `StreamingResponse`
14.6 Integrate mock LLM streaming inside `/chat` (fake tokens)
14.7 Add basic error handling and server-side logging for `/chat`
14.8 Manually exercise `/chat` via curl / browser to verify SSE behavior

## üìÖ Log Entry: November 29, 2025: roadmap changes

While implementing the SSE utilities in Step 14, we realized the roadmap lacked a dedicated step for **structured object streaming**. The gap became clear when defining `event: text` and `event: done`: the backend had no way to emit structured widgets or cards, even though the assistant will eventually generate them. This revealed that the backend transport layer and LangGraph logic were not prepared to output structured objects in a validated, streamable format.

To fix this without renumbering the roadmap, we inserted a new step **20.5** between Steps 20 and 21. This is the correct location because Step 20 completes the basic text-streaming pipeline, and Step 21 focuses on frontend integration. Step 20.5 adds all backend and assistant capabilities required to produce structured objects before exposing them to the frontend.

In Step 29, we originally planned only UI/UX polish. While mapping out the event system for objects, we also recognized the need for a dedicated mechanism to stream warnings or recoverable errors from the backend. This led to adding **29b (Error-Recovery and Retry Handling)** and **29c (`event: alert` support)** so the frontend can handle assistant-side issues gracefully and consistently.

### Updated Roadmap

1. Install and set up VSCode
2. Install and set up Cursor
3. Create repo and initialize Git workspace
4. Scaffold frontend with Vite + React + TypeScript
5. Install and configure Tailwind CSS v4
6. Install and configure shadcn/ui (Tailwind v4 compatible)
   6b. Set up shadcn MCP server for Cursor integration
7. Build global UI shell (header, container, chat layout)
8. Implement chat message components and input bar
9. Add SSE client hook (EventSource) with polling fallback
10. Draft custom widgets directly in Cursor
11. Refactor widgets to shadcn/ui conventions
12. Scaffold FastAPI backend with Uvicorn and CORS
13. Define Pydantic settings and environment management
14. Implement `/chat` SSE endpoint (token streaming)
15. Implement `/summary` endpoint (JSON)
16. Wire LLM provider
17. Build LangGraph ‚Äúinterview ‚Üí summary‚Äù flow
18. Design SQLModel models
19. Implement persistence services
20. Stream-and-save messages during `/chat`

20.5 Add structured object streaming support
20.5.1 Add `event: object` SSE formatter
20.5.2 Define Pydantic object payload models
20.5.3 Extend LangGraph to output structured objects
20.5.4 Convert structured objects to SSE events
20.5.5 Basic tests (optional)
20.5.6 Document object event protocol

21. Connect frontend to backend endpoints
22. Add anonymous session handling
23. Configure local dev (proxy, CORS, env vars)
24. Write minimal tests
25. Deploy frontend
26. Deploy backend
27. Production config (CORS, HTTPS, SSE stability)
28. Observability (logs, request IDs, monitoring)
29. Polish UI/UX
    29b. Error-Recovery and Retry Handling
    29c. Add `event: alert` support
30. Security hardening

## üìÖ Log Entry: November 29, 2025: step 14 Done

**Step 14 ‚Äì `/chat` SSE Endpoint Completed**

We finished implementing the complete backend streaming system for the `/chat` endpoint. This included defining schemas, setting up in-memory session state, building a non-streaming handler, adding SSE utilities, wiring the FastAPI route, introducing realistic async token streaming, and adding minimal failure handling. We also debugged a Windows-specific Uvicorn reload issue where a stale import state caused `/health` to hang; forcing a clean reload resolved it. The final system now streams tokens asynchronously, logs failures, emits a controlled error event when issues occur, and always ends with `event: done`, ensuring the frontend closes connections reliably.

All tests‚Äîcurl, browser DevTools, and frontend UI‚Äîconfirmed correct behavior in both normal and failure modes. This completes the backend foundation required before LLM integration and LangGraph logic.

### New and Updated Files Added in Step 14

```
backend/
  app/
    routers/
      chat.py                 # Defines the POST /chat SSE endpoint using StreamingResponse.
    schemas/
      chat.py                 # Pydantic models for ChatRequest, ChatStreamChunk, ChatFinalResponse.
    services/
      chat_service.py         # Core chat logic: stores messages, creates mock assistant reply, async mock streamer.
    sse/
      stream_utils.py         # SSE helper functions for formatting text events and done events.
    state/
      session_state.py        # In-memory session store with SessionState + global SESSIONS dict.
```

**What each does:**

- **`routers/chat.py`** ‚Äì Implements the `/chat` route. Wraps the async generator, streams tokens via SSE, handles errors, and sends `event: done` on all paths.
- **`schemas/chat.py`** ‚Äì Defines the request and response models used by `/chat`.
- **`services/chat_service.py`** ‚Äì Provides `handle_chat_message` (session updates + mock reply) and `mock_stream_reply` (async token generator with delays).
- **`sse/stream_utils.py`** ‚Äì Formats SSE messages (`event: text` and `event: done`) and provides basic text-stream helpers.
- **`state/session_state.py`** ‚Äì Stores conversation state in memory through `SessionState` and the global `SESSIONS` dictionary.

Step 14 is complete.

### sub-steps for step 15. Next steps...

15.1 Define Pydantic request/response models for `/summary`
15.2 Create service function that assembles the summary (mock implementation for now)
15.3 Implement POST `/summary` route in FastAPI
15.4 Add backend-side validation and error handling
15.5 Manually test the endpoint with curl/Postman

## Log Entry: November 29, 2025: step 15 Done.

**Step 15 ‚Äì `/summary` JSON Endpoint Completed**

We implemented the complete `/summary` endpoint as a non-streaming JSON API. This step established the contract for generating a finalized summary object that the frontend or other services can consume deterministically. The work included creating Pydantic models (`SummaryRequest`, `SummaryResponse`), adding a mock summary generator in `summary_service.py`, implementing the POST endpoint, adding validation and structured error handling, and wiring the router into the FastAPI application. Manual testing with curl confirmed correct status codes, request validation, and JSON responses. Unlike `/chat`, this endpoint intentionally does not stream output; summaries are treated as final artifacts rather than incremental token streams.

### New and Updated Files Added in Step 15

```
backend/
  app/
    routers/
      summary.py              # POST /summary endpoint with validation and JSON response.
    schemas/
      summary.py              # SummaryRequest and SummaryResponse Pydantic models.
    services/
      summary_service.py      # Mock summary generator built on session state.
```

**What each does:**

- **`schemas/summary.py`** ‚Äì Defines typed request/response models and ensures proper OpenAPI documentation.
- **`services/summary_service.py`** ‚Äì Produces a deterministic mock summary; will later delegate to an LLM.
- **`routers/summary.py`** ‚Äì Validates the request, handles errors, and returns a structured JSON `SummaryResponse`.

Step 15 is complete.
